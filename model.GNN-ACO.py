import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv
from torch_geometric.data import Data
import numpy as np
from collections import Counter
import time
import textwrap
import pandas as pd
import os
import logging
import csv
from tqdm import tqdm

# --- Ph·∫ßn 0: C·∫•u h√¨nh Logging ---
# Ghi l·∫°i c√°c th√¥ng tin, c·∫£nh b√°o v√† l·ªói trong qu√° tr√¨nh ch·∫°y v√†o file cutting.log
logging.basicConfig(filename='cutting.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Ph·∫ßn 1: ƒê·ªãnh nghƒ©a v√† c√°c h√†m li√™n quan ƒë·∫øn GNN ---
class GNN_Heuristic(nn.Module):
    """
    ƒê·ªãnh nghƒ©a ki·∫øn tr√∫c M·∫°ng N∆°-ron ƒê·ªì th·ªã (GNN).
    M·∫°ng n√†y h·ªçc m·ªëi quan h·ªá gi·ªØa c√°c s·∫£n ph·∫©m ƒë·ªÉ ƒë∆∞a ra g·ª£i √Ω (heuristic)
    cho thu·∫≠t to√°n ACO v·ªÅ vi·ªác s·∫£n ph·∫©m n√†o n√™n ƒë∆∞·ª£c c·∫Øt c√πng nhau.
    """
    def __init__(self, in_channels, hidden_channels, embedding_dim):
        super(GNN_Heuristic, self).__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=4)
        self.conv2 = GATConv(hidden_channels * 4, embedding_dim, heads=1)
        self.fc = nn.Linear(embedding_dim * 2, 1)

    def forward(self, x, edge_index, edge_label_index=None):
        # Lan truy·ªÅn ti·∫øn qua c√°c l·ªõp GATConv v·ªõi h√†m k√≠ch ho·∫°t elu v√† dropout
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)

        # N·∫øu ƒëang trong qu√° tr√¨nh hu·∫•n luy·ªán, t√≠nh to√°n x√°c su·∫•t cho c√°c c·∫°nh
        if edge_label_index is not None:
            # Gh√©p n·ªëi c√°c vector embedding c·ªßa c√°c c·∫∑p n√∫t
            edge_features = torch.cat([x[edge_label_index[0]], x[edge_label_index[1]]], dim=-1)
            # Tr·∫£ v·ªÅ x√°c su·∫•t c·∫°nh (0 ƒë·∫øn 1) qua l·ªõp linear v√† h√†m sigmoid
            return torch.sigmoid(self.fc(edge_features))
        
        # N·∫øu ƒëang trong qu√° tr√¨nh suy lu·∫≠n, tr·∫£ v·ªÅ vector embedding c·ªßa c√°c n√∫t
        return x

def update_and_save_patterns(plan, items, file_path='du_lieu_cat.csv'):
    """
    C·∫≠p nh·∫≠t file csv ch·ª©a c√°c m·∫´u c·∫Øt t·ªëi ∆∞u.
    L∆∞u d∆∞·ªõi d·∫°ng c√°c t√™n s·∫£n ph·∫©m ph√¢n c√°ch b·ªüi d·∫•u ph·∫©y.
    """
    if not plan:
        return
    print(f"\nüíæ C·∫≠p nh·∫≠t kho d·ªØ li·ªáu m·∫´u c·∫Øt t·∫°i '{file_path}'...")
    length_to_name = {v: k for k, v in items.items()}
    existing_patterns = set()
    try:
        with open(file_path, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            # B·ªè qua header
            try: next(reader)
            except StopIteration: pass
            for row in reader:
                if row: existing_patterns.add(row[0])
    except FileNotFoundError:
        print(f"   -> File '{file_path}' ch∆∞a t·ªìn t·∫°i, s·∫Ω ƒë∆∞·ª£c t·∫°o m·ªõi.")
        pass

    new_patterns_to_add = set()
    for p_info in plan:
        pattern_names = [length_to_name.get(length, 'UNKNOWN') for length in p_info['pattern']]
        canonical_form = ",".join(sorted(pattern_names))
        
        if canonical_form not in existing_patterns:
            new_patterns_to_add.add(canonical_form)

    if new_patterns_to_add:
        try:
            # M·ªü file ·ªü ch·∫ø ƒë·ªô 'a' (append), t·∫°o n·∫øu ch∆∞a c√≥
            is_new_file = not os.path.exists(file_path) or os.path.getsize(file_path) == 0
            with open(file_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                if is_new_file:
                    writer.writerow(['pattern_names_csv'])
                
                for pattern_str in sorted(list(new_patterns_to_add)):
                     writer.writerow([pattern_str])
            print(f"   -> ƒê√£ th√™m {len(new_patterns_to_add)} m·∫´u c·∫Øt m·ªõi v√†o kho d·ªØ li·ªáu.")
        except IOError as e:
            logging.error(f"Kh√¥ng th·ªÉ ghi v√†o file {file_path}: {e}")
            print(f"L·ªñI: Kh√¥ng th·ªÉ ghi v√†o file {file_path}.")
    else:
        print("   -> Kh√¥ng c√≥ m·∫´u c·∫Øt m·ªõi n√†o ƒë·ªÉ th√™m.")

def generate_training_data(items, demands, stock_length, num_samples=2000, historical_patterns_path='du_lieu_cat.csv'):
    """
    T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán GNN t·ª´ file l·ªãch s·ª≠ v√† d·ªØ li·ªáu ng·∫´u nhi√™n.
    ƒê·ªçc d·ªØ li·ªáu t·ª´ c·∫•u tr√∫c m·ªõi (ph√¢n c√°ch b·ªüi d·∫•u ph·∫©y).
    """
    item_lengths_arr = np.array(list(items.values()))
    n_items = len(items)
    item_names = list(items.keys())
    name_to_idx = {name: i for i, name in enumerate(item_names)}

    # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng c·ªßa n√∫t (chi·ªÅu d√†i v√† s·∫£n l∆∞·ª£ng)
    norm_lengths = item_lengths_arr / stock_length
    max_demand = np.max(list(demands.values())) if demands else 1
    norm_demands = np.array([demands.get(name, 0) for name in item_names]) / max_demand if max_demand > 0 else np.zeros(n_items)
    node_features = np.vstack([norm_lengths, norm_demands]).T
    
    edge_list = [[i, j] for i in range(n_items) for j in range(n_items) if i != j]
    edge_index_tensor = torch.tensor(edge_list, dtype=torch.long).t().contiguous()

    data_list = []
    all_source_patterns = []

    # ƒê·ªçc d·ªØ li·ªáu l·ªãch s·ª≠
    try:
        with open(historical_patterns_path, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader) # B·ªè qua header
            historical_patterns = []
            for row in reader:
                if not row: continue
                pattern_names = row[0].split(',')
                pattern_indices = [name_to_idx[name] for name in pattern_names if name in name_to_idx]
                if pattern_indices:
                    historical_patterns.append(pattern_indices)
            all_source_patterns.extend(historical_patterns)
        if historical_patterns:
            print(f"   -> ƒê√£ ƒë·ªçc {len(historical_patterns)} m·∫´u c·∫Øt t·ª´ '{historical_patterns_path}' ƒë·ªÉ l√†m d·ªØ li·ªáu hu·∫•n luy·ªán.")
    except (FileNotFoundError, StopIteration):
        print(f"   -> Kh√¥ng t√¨m th·∫•y ho·∫∑c file '{historical_patterns_path}' tr·ªëng. S·∫Ω d√πng d·ªØ li·ªáu ng·∫´u nhi√™n.")
    except Exception as e:
        print(f"   -> L·ªói khi ƒë·ªçc file l·ªãch s·ª≠ {historical_patterns_path}: {e}. B·ªè qua.")

    # Sinh ng·∫´u nhi√™n d·ªØ li·ªáu ƒë·ªÉ b·ªï sung
    num_random_samples = max(0, num_samples - len(all_source_patterns))
    if num_random_samples > 0:
        print(f"   -> S·∫Ω t·∫°o th√™m {num_random_samples} m·∫´u ng·∫´u nhi√™n ƒë·ªÉ ƒë·ªß {num_samples} m·∫´u.")
        for _ in range(num_random_samples):
            remaining_length = stock_length
            pattern = []
            possible_indices = list(range(n_items))
            np.random.shuffle(possible_indices)
            for idx in possible_indices:
                if item_lengths_arr[idx] <= remaining_length and demands.get(item_names[idx], 0) > 0:
                    pattern.append(idx)
                    remaining_length -= item_lengths_arr[idx]
            all_source_patterns.append(pattern)
            
    # T·∫°o ƒë·ªëi t∆∞·ª£ng Data cho t·∫•t c·∫£ c√°c m·∫´u
    for pattern in all_source_patterns:
        edge_labels = torch.zeros(len(edge_list), dtype=torch.float)
        if len(pattern) > 1:
            for i in range(len(pattern)):
                for j in range(i + 1, len(pattern)):
                    u, v = pattern[i], pattern[j]
                    try:
                        edge_idx = edge_list.index([u, v]) if [u, v] in edge_list else edge_list.index([v, u])
                        edge_labels[edge_idx] = 1.0
                    except ValueError:
                        continue
        data = Data(x=torch.tensor(node_features, dtype=torch.float), edge_index=edge_index_tensor, edge_label_index=edge_index_tensor, edge_label=edge_labels)
        data_list.append(data)
    
    return data_list

def train_gnn(model, data_list, epochs=50, lr=0.01):
    """
    H√†m hu·∫•n luy·ªán m√¥ h√¨nh GNN v·ªõi hi·ªÉn th·ªã ti·∫øn tr√¨nh chi ti·∫øt.
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCELoss()
    model.train()
    
    print("\nB·∫Øt ƒë·∫ßu hu·∫•n luy·ªán GNN...")
    if not data_list:
        print("C·∫¢NH B√ÅO: Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán. B·ªè qua b∆∞·ªõc hu·∫•n luy·ªán.")
        return model

    training_start_time = time.time()
    
    for epoch in range(epochs):
        epoch_start_time = time.time()
        total_loss = 0
        
        # S·ª≠ d·ª•ng tqdm ƒë·ªÉ t·∫°o thanh ti·∫øn tr√¨nh cho m·ªói epoch
        data_iterator = tqdm(data_list, desc=f"Epoch {epoch+1}/{epochs}", leave=False, ncols=100)
        
        for data in data_iterator:
            optimizer.zero_grad()
            out = model(data.x, data.edge_index, data.edge_label_index).squeeze()
            
            if out.shape != data.edge_label.shape:
                 logging.warning(f"B·ªè qua m·∫´u do l·ªói shape: out: {out.shape}, label: {data.edge_label.shape}")
                 continue

            loss = criterion(out, data.edge_label)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            data_iterator.set_postfix(loss=f"{loss.item():.4f}")

        avg_loss = total_loss / len(data_list) if len(data_list) > 0 else 0
        epoch_duration = time.time() - epoch_start_time
        
        print(f"Epoch {epoch+1}/{epochs} ho√†n th√†nh | Loss trung b√¨nh: {avg_loss:.4f} | Th·ªùi gian: {epoch_duration:.2f} gi√¢y")

    total_training_time = time.time() - training_start_time
    
    print("-" * 60)
    print(f"‚úÖ Hu·∫•n luy·ªán GNN ho√†n t·∫•t!")
    print(f"   -> T·ªïng th·ªùi gian hu·∫•n luy·ªán: {total_training_time:.2f} gi√¢y.")
    print("-" * 60)
    
    return model

def save_gnn_model(model, path='gnn_model.pt'):
    """L∆∞u tr·ªçng s·ªë c·ªßa m√¥ h√¨nh GNN ƒë√£ hu·∫•n luy·ªán."""
    torch.save(model.state_dict(), path)
    print(f"ƒê√£ l∆∞u m√¥ h√¨nh GNN t·∫°i '{path}'")

# --- Ph·∫ßn 2: ƒê·ªãnh nghƒ©a Solver lai gh√©p GNN-ACO ---
class GNN_ACO_Solver:
    """
    L·ªõp ch√≠nh ƒë·ªÉ gi·∫£i b√†i to√°n, k·∫øt h·ª£p GNN v√† ACO.
    - GNN: Cung c·∫•p ma tr·∫≠n heuristic (g·ª£i √Ω).
    - ACO: S·ª≠ d·ª•ng ma tr·∫≠n heuristic v√† pheromone ƒë·ªÉ x√¢y d·ª±ng c√°c gi·∫£i ph√°p.
    """
    def __init__(self, stock_length, items, demands, gnn_model, aco_params):
        self.stock_length = stock_length
        self.item_lengths = np.array(list(items.values()))
        self.item_indices = {name: i for i, name in enumerate(items.keys())}
        self.index_to_name = {i: name for name, i in self.item_indices.items()}
        self.demands_initial = np.array([demands.get(self.index_to_name[i], 0) for i in range(len(items))])
        self.n_items = len(items)
        self.params = aco_params
        
        self.gnn_model = gnn_model
        self.heuristic_matrix = self._calculate_heuristic_info()
        self.pheromone_matrix = np.ones((self.n_items, self.n_items))
        
        self.best_overall_plan = None
        self.best_overall_waste = float('inf')
        self.best_overall_fitness = float('inf')

    def _create_graph_from_items(self):
        """T·∫°o ƒë·ªëi t∆∞·ª£ng ƒë·ªì th·ªã t·ª´ d·ªØ li·ªáu b√†i to√°n hi·ªán t·∫°i cho GNN."""
        norm_lengths = self.item_lengths / self.stock_length
        max_demand = np.max(self.demands_initial)
        norm_demands = self.demands_initial / max_demand if max_demand > 0 else np.zeros_like(self.demands_initial, dtype=float)
        node_features = np.vstack([norm_lengths, norm_demands]).T
        
        edge_list = [[i, j] for i in range(self.n_items) for j in range(self.n_items) if i != j]
        x = torch.tensor(node_features, dtype=torch.float)
        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        return Data(x=x, edge_index=edge_index)

    def _calculate_heuristic_info(self):
        """S·ª≠ d·ª•ng GNN ƒë·ªÉ suy lu·∫≠n v√† t·∫°o ra ma tr·∫≠n heuristic."""
        print("\nGNN ƒëang t√≠nh to√°n ma tr·∫≠n heuristic...")
        graph = self._create_graph_from_items()
        self.gnn_model.eval()
        with torch.no_grad():
            embeddings = self.gnn_model(graph.x, graph.edge_index).cpu().numpy()
        
        heuristic_matrix = np.dot(embeddings, embeddings.T)
        min_val, max_val = np.min(heuristic_matrix), np.max(heuristic_matrix)
        if max_val == min_val:
            return np.ones_like(heuristic_matrix)
        return (heuristic_matrix - min_val) / (max_val - min_val + 1e-9)

    def _build_one_pattern(self, demands):
        """M·ªôt con ki·∫øn x√¢y d·ª±ng m·ªôt m·∫´u c·∫Øt."""
        pattern_indices = []
        remaining_length = self.stock_length
        
        possible_first = np.where((demands > 0) & (self.item_lengths <= remaining_length))[0]
        if len(possible_first) == 0:
            return [], []
        
        first_item_idx = np.random.choice(possible_first)
        pattern_indices.append(first_item_idx)
        remaining_length -= self.item_lengths[first_item_idx]
        
        while True:
            last_item_idx = pattern_indices[-1]
            possible_next = np.where((demands > 0) & (self.item_lengths <= remaining_length))[0]
            
            if len(possible_next) == 0:
                break
                
            pheromones = self.pheromone_matrix[last_item_idx, possible_next]
            heuristics = self.heuristic_matrix[last_item_idx, possible_next]
            probabilities = (pheromones ** self.params['alpha']) * (heuristics ** self.params['beta'])
            
            prob_sum = np.sum(probabilities)
            if prob_sum <= 1e-9 or not np.all(np.isfinite(probabilities)):
                probabilities = np.ones(len(possible_next)) / len(possible_next)
            else:
                probabilities /= prob_sum
            
            next_item_idx = np.random.choice(possible_next, p=probabilities)
            pattern_indices.append(next_item_idx)
            remaining_length -= self.item_lengths[next_item_idx]
        
        pattern_lengths = [self.item_lengths[i] for i in pattern_indices]
        return pattern_lengths, pattern_indices

    def _calculate_pattern_repeats(self, pattern_indices, demands):
        """T√≠nh s·ªë l·∫ßn l·∫∑p l·∫°i t·ªëi ƒëa cho m·ªôt m·∫´u c·∫Øt ƒë·ªÉ kh√¥ng v∆∞·ª£t qu√° s·∫£n l∆∞·ª£ng y√™u c·∫ßu."""
        if not pattern_indices:
            return 0
        
        counts = Counter(pattern_indices)
        max_repeats_options = [int(demands[idx] // count) for idx, count in counts.items() if demands[idx] > 0 and count > 0]
        
        return max(1, min(max_repeats_options)) if max_repeats_options else 0

    def _construct_full_cutting_plan(self):
        """M·ªôt con ki·∫øn x√¢y d·ª±ng m·ªôt k·∫ø ho·∫°ch c·∫Øt ho√†n ch·ªânh ƒë·ªÉ ƒë√°p ·ª©ng t·∫•t c·∫£ y√™u c·∫ßu."""
        remaining_demands = self.demands_initial.copy()
        cutting_plan = []
        produced_counts = np.zeros(self.n_items)
        
        while np.sum(remaining_demands[remaining_demands > 0]) > 0:
            new_pattern_lengths, new_pattern_indices = self._build_one_pattern(remaining_demands)
            
            if not new_pattern_lengths:
                break 
            
            repeats = self._calculate_pattern_repeats(new_pattern_indices, remaining_demands)
            
            if repeats == 0:
                for idx in set(new_pattern_indices):
                    remaining_demands[idx] = 0 
                continue

            cutting_plan.append({'pattern': new_pattern_lengths, 'repeats': repeats, 'indices': new_pattern_indices})
            counts_in_pattern = Counter(new_pattern_indices)
            for item_idx, count in counts_in_pattern.items():
                produced_counts[item_idx] += count * repeats
                remaining_demands[item_idx] = max(0, remaining_demands[item_idx] - count * repeats)

        for item_idx in range(self.n_items):
            shortfall = self.demands_initial[item_idx] - produced_counts[item_idx]
            if shortfall > 0:
                item_length = self.item_lengths[item_idx]
                if item_length <= self.stock_length:
                    items_per_bar = self.stock_length // item_length
                    num_bars_needed = int(np.ceil(shortfall / items_per_bar))
                    
                    cutting_plan.append({
                        'pattern': [item_length] * int(items_per_bar),
                        'repeats': num_bars_needed,
                        'indices': [item_idx] * int(items_per_bar)
                    })
                    produced_counts[item_idx] += items_per_bar * num_bars_needed

        total_waste = sum([(self.stock_length - sum(p['pattern'])) * p['repeats'] for p in cutting_plan])
        
        unmet_demands = np.sum(np.maximum(0, self.demands_initial - produced_counts))
        penalty = self.params.get('penalty_weight', 1e6) * unmet_demands
        fitness = total_waste + penalty
        
        return cutting_plan, total_waste, fitness

    def _update_pheromones(self, all_ant_plans):
        """C·∫≠p nh·∫≠t ma tr·∫≠n pheromone sau m·ªói th·∫ø h·ªá."""
        self.pheromone_matrix *= (1 - self.params['evaporation_rate'])
        
        for plan, waste, fitness in all_ant_plans:
            if plan is None:
                continue
            
            deposit_amount = 1.0 / (fitness + 1.0)
            
            for pattern_info in plan:
                indices = pattern_info['indices']
                for i in range(len(indices) - 1):
                    for j in range(i + 1, len(indices)):
                        start_node, end_node = indices[i], indices[j]
                        self.pheromone_matrix[start_node, end_node] += deposit_amount
                        self.pheromone_matrix[end_node, start_node] += deposit_amount

    def solve(self):
        """H√†m ch√≠nh ƒë·ªÉ ch·∫°y thu·∫≠t to√°n GNN-ACO."""
        print("\nüöÄ B·∫Øt ƒë·∫ßu gi·∫£i b√†i to√°n b·∫±ng GNN-ACO...")
        for gen in range(self.params['max_generations']):
            all_ant_plans = []
            for _ in range(self.params['num_ants']):
                plan, waste, fitness = self._construct_full_cutting_plan()
                all_ant_plans.append((plan, waste, fitness))
            
            best_in_gen_plan, _, best_in_gen_fitness = min(all_ant_plans, key=lambda x: x[2])
            
            if best_in_gen_fitness < self.best_overall_fitness:
                self.best_overall_fitness = best_in_gen_fitness
                self.best_overall_plan = best_in_gen_plan
                # T√≠nh l·∫°i waste ch√≠nh x√°c cho gi·∫£i ph√°p t·ªët nh·∫•t (v√¨ fitness c√≥ th·ªÉ ch·ª©a penalty)
                self.best_overall_waste = sum([(self.stock_length - sum(p['pattern'])) * p['repeats'] for p in self.best_overall_plan])
                
                is_valid = (self.best_overall_fitness - self.best_overall_waste) < 1.0
                valid_str = "‚úÖ H·ª£p l·ªá" if is_valid else "‚ùå Ch∆∞a h·ª£p l·ªá"
                print(f"Th·∫ø h·ªá {gen+1:02d}: üî• Gi·∫£i ph√°p m·ªõi! L√£ng ph√≠: {self.best_overall_waste:.2f} (Fitness: {self.best_overall_fitness:.2f}) - {valid_str}")

            self._update_pheromones(all_ant_plans)
            
            if (gen + 1) % 10 == 0:
                print(f"   -> Ho√†n th√†nh th·∫ø h·ªá {gen+1}/{self.params['max_generations']}. Fitness t·ªët nh·∫•t hi·ªán t·∫°i: {self.best_overall_fitness:.2f}")
        
        print("\n‚úÖ Gi·∫£i xong!")
        return self.best_overall_plan, self.best_overall_waste

# --- Ph·∫ßn 3: H√†m in k·∫øt qu·∫£ ---
def print_beautiful_results(plan, waste, stock_length, execution_time, items, demands):
    """In k·∫øt qu·∫£ cu·ªëi c√πng ra m√†n h√¨nh m·ªôt c√°ch tr·ª±c quan."""
    stt_col_width, pattern_col_width, repeats_col_width, waste_col_width = 4, 45, 10, 15
    total_width = stt_col_width + pattern_col_width + repeats_col_width + waste_col_width + 13

    print("\n" + "="*total_width)
    print("||" + " B√ÅO C√ÅO K·∫æT QU·∫¢ T·ªêI ∆ØU H√ìA C·∫ÆT TH√âP ".center(total_width - 4) + "||")
    print("="*total_width)

    if not plan:
        print("\nKh√¥ng t√¨m th·∫•y gi·∫£i ph√°p n√†o.".center(total_width))
        print("="*total_width)
        return

    # G·ªôp c√°c m·∫´u c·∫Øt gi·ªëng h·ªát nhau
    unique_patterns = {}
    for p_info in plan:
        pattern_tuple = tuple(sorted(p_info['pattern'], reverse=True))
        if pattern_tuple not in unique_patterns:
            unique_patterns[pattern_tuple] = 0
        unique_patterns[pattern_tuple] += p_info['repeats']

    total_bars = sum(unique_patterns.values())
    total_stock_material = total_bars * stock_length
    total_used_material = sum(sum(p) * r for p, r in unique_patterns.items())
    actual_waste = total_stock_material - total_used_material
    efficiency = (total_used_material / total_stock_material) * 100 if total_stock_material > 0 else 0
    
    print("\nüìä [ B·∫¢NG T√ìM T·∫ÆT T·ªîNG QUAN ]\n")
    print(f"   - T·ªïng s·ªë thanh th√©p g·ªëc s·ª≠ d·ª•ng   : {total_bars} thanh")
    print(f"   - Hi·ªáu su·∫•t s·ª≠ d·ª•ng v·∫≠t li·ªáu       : {efficiency:.2f} %")
    print(f"   - T·ªïng l∆∞·ª£ng l√£ng ph√≠              : {actual_waste:.2f}")
    print(f"   - Th·ªùi gian th·ª±c thi to√†n b·ªô       : {execution_time:.4f} gi√¢y")

    print("\nüìã [ B·∫¢NG CHI TI·∫æT K·∫æ HO·∫†CH C·∫ÆT ]\n")
    header = f"| {'STT':^{stt_col_width}} | {'M·∫™U C·∫ÆT (C√ÅC S·∫¢N PH·∫®M)':^{pattern_col_width}} | {'L·∫∂P L·∫†I':^{repeats_col_width}} | {'L√ÉNG PH√ç/THANH':^{waste_col_width}} |"
    separator = "-" * len(header)
    print(separator)
    print(header)
    print(separator)

    sorted_patterns = sorted(unique_patterns.items(), key=lambda item: sum(item[0]), reverse=True)
    for i, (pattern, count) in enumerate(sorted_patterns):
        pattern_str = f"({', '.join(map(str, pattern))})"
        waste_per_bar = stock_length - sum(pattern)
        wrapped_lines = textwrap.wrap(pattern_str, width=pattern_col_width - 2)
        
        first_line = wrapped_lines[0] if wrapped_lines else ''
        print(f"| {i+1:<{stt_col_width}} | {first_line:<{pattern_col_width}} | {count:^{repeats_col_width}} | {waste_per_bar:^{waste_col_width}.2f} |")
        
        for line in wrapped_lines[1:]:
            print(f"| {'':<{stt_col_width}} | {line:<{pattern_col_width}} | {'':<{repeats_col_width}} | {'':<{waste_col_width}} |")
    print(separator)

    print("\nüì¶ [ B·∫¢NG T·ªîNG H·ª¢P S·∫¢N L∆Ø·ª¢NG ]\n")
    prod_header = f"| {'S·∫¢N PH·∫®M':<20} | {'Y√äU C·∫¶U (C·∫ßn)':^15} | {'S·∫¢N XU·∫§T (C·∫Øt ƒë∆∞·ª£c)':^20} | {'CH√äNH L·ªÜCH':^15} |"
    prod_separator = "-" * len(prod_header)
    print(prod_separator)
    print(prod_header)
    print(prod_separator)

    length_to_name = {v: k for k, v in items.items()}
    produced_counts = {name: 0 for name in demands.keys()}
    for pattern_tuple, repeats in unique_patterns.items():
        counts_in_pattern = Counter(pattern_tuple)
        for length, num_in_pattern in counts_in_pattern.items():
            if length in length_to_name:
                produced_counts[length_to_name[length]] += num_in_pattern * repeats
            
    for name, required in sorted(demands.items(), key=lambda x: items[x[0]], reverse=True):
        produced = produced_counts.get(name, 0)
        diff = produced - required
        status = "‚úÖ" if diff >= 0 else "‚ùå"
        print(f"| {name:<20} | {required:^15} | {produced:^20} | {f'{diff:+.0f}':^15} {status}|")
    print(prod_separator)
    print("\n" + "="*total_width)

# --- Ph·∫ßn 4: H√†m Main ƒë·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh ---
if __name__ == '__main__':
    script_start_time = time.time()

    print("ƒêang ƒë·ªçc ƒë∆°n h√†ng t·ª´ file don_hang.csv...")
    try:
        order_df = pd.read_csv('don_hang.csv')
        if any(order_df['chieu_dai'] <= 0) or any(order_df['chieu_dai'] > 100.0):
            raise ValueError("Chi·ªÅu d√†i s·∫£n ph·∫©m ph·∫£i l·ªõn h∆°n 0 v√† kh√¥ng qu√° 100.0.")
        if any(order_df['so_luong'] < 0):
            raise ValueError("S·ªë l∆∞·ª£ng y√™u c·∫ßu kh√¥ng ƒë∆∞·ª£c l√† s·ªë √¢m.")
        print("ƒê·ªçc ƒë∆°n h√†ng th√†nh c√¥ng!")
    except FileNotFoundError:
        print("L·ªñI: Kh√¥ng t√¨m th·∫•y file 'don_hang.csv'. Vui l√≤ng t·∫°o file n√†y.")
        exit()
    except ValueError as e:
        print(f"L·ªñI D·ªÆ LI·ªÜU: {e}")
        exit()

    # --- Kh·ªüi t·∫°o c√°c bi·∫øn to√†n c·ª•c ---
    ITEMS = pd.Series(order_df.chieu_dai.values, index=order_df.ten_san_pham).to_dict()
    DEMANDS = pd.Series(order_df.so_luong.values, index=order_df.ten_san_pham).to_dict()

    STOCK_LENGTH = 100.0
    ACO_PARAMS = {
        'num_ants': 20,
        'max_generations': 100,
        'alpha': 1.0,           # T·∫ßm quan tr·ªçng c·ªßa pheromone
        'beta': 3.0,            # T·∫ßm quan tr·ªçng c·ªßa heuristic (GNN)
        'evaporation_rate': 0.5,# T·ªëc ƒë·ªô bay h∆°i
        'penalty_weight': 1e6   # Tr·ªçng s·ªë ph·∫°t n·∫øu kh√¥ng ƒë·ªß s·∫£n l∆∞·ª£ng
    }

    print("\n" + "="*60)
    print(" B√ÄI TO√ÅN T·ªêI ∆ØU H√ìA C·∫ÆT TH√âP 1D (GNN-ACO)".center(60))
    print(f"Chi·ªÅu d√†i thanh th√©p g·ªëc: {STOCK_LENGTH}".center(60))
    print("="*60)

    # --- X·ª≠ l√Ω m√¥ h√¨nh GNN ---
    gnn_model = GNN_Heuristic(in_channels=2, hidden_channels=32, embedding_dim=16)
    model_path = 'gnn_model.pt'
    cutting_data_path = 'du_lieu_cat.csv'

    # Logic t·∫£i ho·∫∑c hu·∫•n luy·ªán GNN
    # N·∫øu mu·ªën hu·∫•n luy·ªán l·∫°i t·ª´ ƒë·∫ßu, h√£y x√≥a file 'gnn_model.pt'
    if os.path.exists(model_path):
        print(f"\n‚úÖ ƒê√£ t√¨m th·∫•y m√¥ h√¨nh GNN t·∫°i '{model_path}'. ƒêang t·∫£i...")
        try:
            gnn_model.load_state_dict(torch.load(model_path))
            gnn_model.eval()
            print("   -> T·∫£i m√¥ h√¨nh th√†nh c√¥ng. B·ªè qua b∆∞·ªõc hu·∫•n luy·ªán.")
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: {e}. S·∫Ω ti·∫øn h√†nh hu·∫•n luy·ªán l·∫°i.")
            if os.path.exists(model_path): os.remove(model_path)
            training_data = generate_training_data(ITEMS, DEMANDS, STOCK_LENGTH)
            gnn_model = train_gnn(gnn_model, training_data, epochs=50)
            save_gnn_model(gnn_model, model_path)
    else:
        print(f"\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh GNN t·∫°i '{model_path}'.")
        training_data = generate_training_data(ITEMS, DEMANDS, STOCK_LENGTH)
        gnn_model = train_gnn(gnn_model, training_data, epochs=50)
        save_gnn_model(gnn_model, model_path)

    # --- Kh·ªüi t·∫°o v√† ch·∫°y Solver ---
    solver = GNN_ACO_Solver(STOCK_LENGTH, ITEMS, DEMANDS, gnn_model, ACO_PARAMS)
    best_plan, best_waste = solver.solve()
    
    script_end_time = time.time()
    execution_time = script_end_time - script_start_time

    # --- In k·∫øt qu·∫£ ---
    print_beautiful_results(best_plan, best_waste, STOCK_LENGTH, execution_time, ITEMS, DEMANDS)
    
    # --- C·∫≠p nh·∫≠t kho d·ªØ li·ªáu ---
    update_and_save_patterns(best_plan, ITEMS, cutting_data_path)